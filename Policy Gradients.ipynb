{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients with Baseline: DOOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create a Policy Gradient based agent that tries to survive in an hostile environement by collecting health.\n",
    "<br>\n",
    "We will use Policy Gradient with Baseline method <b>which is an approach to design the policy gradient algorithm</b>\n",
    "\n",
    "TODO : Add doom gif\n",
    "\n",
    "## Aknowledgements\n",
    "Our implementation will be inspired by this tutorial:\n",
    "https://www.oreilly.com/ideas/reinforcement-learning-with-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A recap: Reinforcement Learning Process üéÆ\n",
    "\n",
    "<img src=\"assets/rl.png\" alt=\"Reinforcement Learning process\"/>\n",
    "\n",
    "Reinforcement Learning: is when an agent learns by interacting with the environement itself (through trial and error) it receives reward when performing correct actions. It's a decision making problem.\n",
    "\n",
    "<br>\n",
    "The Reinforcement Learning loop:\n",
    "\n",
    "- Agent receive state S0 from the environment\n",
    "- Based on that state S0 agent take an action A0\n",
    "- Environnement transitions to a new state S1\n",
    "- Give some reward R1 to the agent\n",
    "<br>\n",
    "<br>\n",
    "‚Üí This output a sequence of state, reward and action.<br>\n",
    "‚Üí The goal of the agent <b>is maximize expected cumulative reward in order to reach an optimal policy (way to behave).</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: What is policy gradient? ü§ñ\n",
    "\n",
    "- Policy Gradient is a policy based reinforcement learning method: in this method, we want <b>to directly to learn an œÄ* by optimize it without worrying about a value function, we‚Äôll directly parameterize the œÄ and do gradient descent into a direction that improves it.</b>\n",
    "<br><br>\n",
    "- Why? Because it's sometimes easier to approximate than the value function. Also, we need a parameterized policy to deal with continuous action spaces and environments where we need to act stochastically.\n",
    "<br><br>\n",
    "- Common choices for the policy function: Softmax for discrete actions, Gaussian parameters for continuous actions.\n",
    "<br><br>\n",
    "- To measure the quality of œÄ, we calculate the objective/score function `J(theta)` : we can use 3 differents methods: start value, average value, or average reward per time step. In our case by calculating the maximum expected cumulative reward (we can use the start value : use the mean of the return from the first time step (G1) == cumulative discounted reward for the entire episode).\n",
    "<br>\n",
    "<img style=\"width: 400px;\" src=\"assets/objective.png\" alt=\"Objective function\"/>\n",
    "<br>\n",
    "<br>\n",
    "- Then, we have an objective function. Now, we want to find a œÄ that max it. We Using gradient ascent by computing the gradients analytically: Policy Gradient Theorem: `grad(J(theta)) = Ex[grad(log(pi(s, a))) * Q(s, a)]`. Basically, we move our policy into a direction of more reward.\n",
    "<br>\n",
    "<img style=\"width: 600px;\" src=\"assets/policygrad.png\" alt=\"Policy gradients\"/>\n",
    "<br>\n",
    "<br>\n",
    "- REINFORCE (Monte Carlo Policy Gradient): We substitute a samples return `g_t` form an episode for Q(s, a) to make an update. Unbiased but high variance.\n",
    "<img src=\"assets/montecarlo.png\" alt=\"Monte Carlo\"/>\n",
    "<br><br>\n",
    "    For each episode:\n",
    "        At each time step within that episode:\n",
    "            Compute the log probabilities produced by our policy function.\n",
    "            Multiply it by the score function.\n",
    "            Update the weights with some small learning rate alpha\n",
    "<br>    <br>     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradients with Baseline method üëæ\n",
    "- Baseline: Instead of measuring the absolute goodness of an action we want to know how much better than \"average\" it is to take an action given a state. E.g. some states are naturally bad and always give negative reward. This is called the advantage and is defined as `Q(s, a) - V(s)`. We use that for our policy update, e.g. `g_t - V(s)` for REINFORCE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's implement our Policy Based Agent üïπÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/doom.gif\" alt=\"Doom game\"/>\n",
    "Environement:\n",
    "- The player is standing on top of acid water.\n",
    "- He needs to learn how to navigate and <b>collect health packs to stay alive</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import skimage\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions ‚öôÔ∏è\n",
    "We have 2 preprocessing functions:\n",
    "- discount: discount rewards and normalize them. This function evaluates a set of rewards from an episode + add normalization to our discounted rewards to make sure our reward range stays small.\n",
    "- preprocess_frame: gray and resize our frame to 84x84x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(r, gamma, normal):\n",
    "    discount = np.zeros_like(r)\n",
    "    G = 0.0\n",
    "    for i in reversed(range(0, len(r))):\n",
    "        G = G * gamma + r[i]\n",
    "        discount[i] = G\n",
    "    # Normalize \n",
    "    if normal:\n",
    "        mean = np.mean(discount)\n",
    "        std = np.std(discount)\n",
    "        discount = (discount - mean) / (std)\n",
    "    return discount\n",
    "\n",
    "\"\"\"# Preprocess the frame\n",
    "## Reshape function:\n",
    "# Processes Doom screen image to produce cropped and resized image. \n",
    "def preprocess_frame(frame):\n",
    "    s = frame[10:-10,30:-30]\n",
    "    s = scipy.misc.imresize(s,[84,84])\n",
    "    s = np.reshape(s,[np.prod(s.shape)]) / 255.0\n",
    "    return s\n",
    "\"\"\"\n",
    "def preprocess_frame(frame):\n",
    "    # Greyscale Frame\n",
    "    x = np.mean(frame,-1)\n",
    "    # Normalize Pixel Values\n",
    "    x = x/255\n",
    "    x = transform.resize(x, [84,84])\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environement üéÆ\n",
    "We use vizdoom that simulate a doom-like game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config(\"health_gathering.cfg\")\n",
    "game.set_doom_scenario_path(\"health_gathering.wad\")\n",
    "game.init()\n",
    "actions_list  = np.identity(3,dtype=bool).tolist()\n",
    "render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model hyperparameters\n",
    "action_size = 3\n",
    "kernel = 8\n",
    "strides = 2\n",
    "padding = \"VALID\"\n",
    "\n",
    "## Tune hyperparameters\n",
    "alpha = 1e-4\n",
    "gamma = 0.99 # Discount rate\n",
    "normalize_reward = True\n",
    "\n",
    "value_scale = 0.5\n",
    "entropy_scale = 0.00\n",
    "gradient_clip = 40\n",
    "\n",
    "## Folders\n",
    "save_path=\"models/healthGather.ckpt\" # Place to save our model\n",
    "episode_path = \"episodes/\"\n",
    "\n",
    "## Training\n",
    "num_epochs = 500\n",
    "batch_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network üß†\n",
    "- Takes as input a state frame.\n",
    "    - Implies preprocess the frame\n",
    "- Output a probability distribution\n",
    "\n",
    "We need as hyperparameters:\n",
    "- action size = nb of probabilities\n",
    "- kernel\n",
    "- strides\n",
    "- padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our placeholders\n",
    "# Our frame is 84x84x1\n",
    "x = tf.placeholder(tf.float32, (None,84,84,1), name=\"inputs\")\n",
    "y = tf.placeholder(tf.int32, (None), name=\"actions\")\n",
    "# Compute the one hot vectors for each action given\n",
    "actions_onehot = tf.one_hot(y ,action_size,dtype=tf.int32)\n",
    "r = tf.placeholder(tf.float32, (None,), name='reward')\n",
    "n = tf.placeholder(tf.float32, (None), name='episodes')\n",
    "d_r = tf.placeholder(tf.float32, (None,), name='discounted_reward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POLICY NETWORK\n",
    "# Input is 128*128*1\n",
    "conv1 = tf.layers.conv2d(inputs = x,\n",
    "                                 filters = 32,\n",
    "                                 kernel_size = kernel,\n",
    "                                 strides = strides,\n",
    "                                 padding = padding,\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name = \"conv1\")\n",
    "        \n",
    "conv1_batchnorm = tf.layers.batch_normalization(conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "\n",
    "conv1_out = tf.nn.elu(conv1_batchnorm, name=\"conv1_out\")\n",
    "# 64x64x32\n",
    "        \n",
    "conv2 = tf.layers.conv2d(inputs = conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = kernel,\n",
    "                                 strides = strides,\n",
    "                                 padding = padding,\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "conv2_batchnorm = tf.layers.batch_normalization(conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "conv2_out = tf.nn.elu(conv2_batchnorm, name=\"conv2_out\")\n",
    "# 32x32x64\n",
    "        \n",
    "        \n",
    "conv3 = tf.layers.conv2d(inputs = conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = kernel,\n",
    "                                 strides = strides,\n",
    "                                 padding = padding,\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "conv3_batchnorm = tf.layers.batch_normalization(conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "conv3_out = tf.nn.elu(conv3_batchnorm, name=\"conv3_out\")\n",
    "# 16x16x128\n",
    "        \n",
    "flatten = tf.layers.flatten(conv3_out)\n",
    "\n",
    "fc1 = tf.layers.dense(inputs = flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                name=\"fc1\")\n",
    "\n",
    "\n",
    "logits = tf.layers.dense(inputs = fc1,\n",
    "                                  units = action_size,\n",
    "                                  activation = None,\n",
    "                        name = \"logits\")\n",
    "\n",
    "value = tf.layers.dense(\n",
    "        inputs=fc1, \n",
    "        units = 1, \n",
    "        name='value')\n",
    "        \n",
    "calc_action = tf.multinomial(logits, 1)\n",
    "aprob = tf.nn.softmax(logits)\n",
    "action_logprob = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1/kernel:0' shape=(8, 8, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_norm1/gamma:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_norm1/beta:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/kernel:0' shape=(8, 8, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/bias:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_norm2/gamma:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_norm2/beta:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv3/kernel:0' shape=(8, 8, 64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'conv3/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_norm3/gamma:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'batch_norm3/beta:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'fc1/kernel:0' shape=(3200, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'fc1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'logits/kernel:0' shape=(512, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'logits/bias:0' shape=(3,) dtype=float32_ref>,\n",
       " <tf.Variable 'value/kernel:0' shape=(512, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'value/bias:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses and gradients ‚õ∞Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Gradient Loss\n",
    "- We defined an objective function  as <b> total reward an agent can achieve under policy œÄ averaged over all starting states </b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "J(\\theta) = E_{\\pi} [G_{1}]\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "J(\\theta) = E_{\\pi} [G_{1}]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also know that a gradient of this function is determined by policy gradient theorem as:\n",
    "<img src=\"assets/pg.png\" alt=\"pg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We‚Äôre trying to maximize the J function so we can just say that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "L_{\\pi} = - J(\\theta)\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "L_{\\pi} = - J(\\theta)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final loss function is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "L_{\\pi} = -1/n \\sum_{i=1}^{n} A(s_{i}, a_{i}) * log\\pi(a_{i} | s_{i}) \n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "L_{\\pi} = -1/n \\sum_{i=1}^{n} A(s_{i}, a_{i}) * log\\pi(a_{i} | s_{i}) \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where: \n",
    "- -1/n sum : tf.reduce_mean\n",
    "- A(si,ai): (d_r - value)\n",
    "- logpi(ai|si): tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) calculates the softmax and log loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tf.reduce_mean([[3.,4], [5.,6], [6.,7]]) <br>\n",
    "--> [ 3.5  5.5  6.5] <br>\n",
    "--> Mean between element <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyGradient_loss = tf.reduce_mean((d_r - value) * tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels=actions_onehot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value loss\n",
    "Calculate our value loss by using common squared mean error loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_loss = value_scale * tf.reduce_mean(tf.square(d_r - value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_loss = -entropy_scale * tf.reduce_sum(aprob * tf.exp(aprob))\n",
    "loss = policyGradient_loss + value_loss - entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(alpha)\n",
    "gradients = tf.gradients(loss, tf.trainable_variables())\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, gradient_clip) # gradient clipping\n",
    "gradients_and_variables = list(zip(gradients, tf.trainable_variables()))\n",
    "train_op = optimizer.apply_gradients(gradients_and_variables)\n",
    "\n",
    "# Initialize Session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training ‚è≤Ô∏è\n",
    "#### Rollout function\n",
    "This function will gather a batch of training data from multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(batch_size, render):\n",
    "    states, actions, rewards, rewardsFeed, discountedRewards = [], [], [], [], []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Launch a new episode\n",
    "    game.new_episode()\n",
    "    state = game.get_state().screen_buffer\n",
    "    state = preprocess_frame(state)\n",
    "    episode_num = 0\n",
    "    \n",
    "    ### ???\n",
    "    action_repeat = 3\n",
    "    reward = 0\n",
    "    \n",
    "    while True:\n",
    "        # Run State Through Policy & Calculate Action\n",
    "        state = state.reshape(1, 84, 84, 1)\n",
    "        feed = {x: state}\n",
    "        action = sess.run(calc_action, feed_dict=feed)\n",
    "        action = action[0][0]\n",
    "        \n",
    "        # Perform action\n",
    "        for i in range(action_repeat):\n",
    "            new_reward = game.make_action(actions_list[action])\n",
    "            done = game.is_episode_finished()\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            reward += new_reward\n",
    "            new_state = game.get_state().screen_buffer\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Store results\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        actions.append(action) \n",
    "        \n",
    "        # Update Current State\n",
    "        reward = 0\n",
    "        state = preprocess_frame(new_state)\n",
    "        if done:\n",
    "            \n",
    "            # Track Discounted Rewards\n",
    "            rewardsFeed.append(rewards)\n",
    "            discountedRewards.append(discount(rewards, gamma, normalize_reward))\n",
    "            \n",
    "            if len(np.concatenate(rewardsFeed)) > batch_size:\n",
    "                break\n",
    "                \n",
    "            # Reset Environment\n",
    "            rewards = []\n",
    "            game.new_episode(\"episodes/episode\" + str(episode_num) + \"_rec.lmp\")\n",
    "            state = game.get_state().screen_buffer\n",
    "            state = preprocess_frame(state)\n",
    "            episode_num += 1\n",
    "                         \n",
    "    return np.stack(states), np.stack(actions), np.concatenate(rewardsFeed), np.concatenate(discountedRewards), episode_num\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summaries üìã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = tf.divide(tf.reduce_sum(r), n)\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Total_Loss\", loss)\n",
    "tf.summary.scalar(\"PolicyGradient_Loss\", policyGradient_loss)\n",
    "tf.summary.scalar(\"Entropy_Loss\", entropy_loss)\n",
    "tf.summary.scalar(\"Value_Loss\", value_loss)\n",
    "\n",
    "## Rewards\n",
    "tf.summary.scalar(\"Mean_Reward\", avg_reward)\n",
    "\n",
    "## Model\n",
    "#tf.summary.histogram('Conv1', tf.trainable_variables()[0])\n",
    "#tf.summary.histogram('Conv2', tf.trainable_variables()[2])\n",
    "#tf.summary.histogram('FC', tf.trainable_variables()[4])\n",
    "#tf.summary.histogram('Logits', tf.trainable_variables()[6])\n",
    "#tf.summary.histogram('Value', tf.trainable_variables()[8])\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 / 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\Anaconda3\\envs\\gameplai\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Episodes: Tensor(\"episodes:0\", dtype=float32)  Average Reward: 555.56  Total Average: 555.56\n"
     ]
    }
   ],
   "source": [
    "average = []\n",
    "step = 0\n",
    "epoch = 0\n",
    "while epoch < num_epochs + 1:\n",
    "    # Gather training data\n",
    "    print(\"Epoch \", epoch, \"/\", num_epochs)\n",
    "    s_, a_, r_, d_r_, n_ = rollout(batch_size, render)\n",
    "    \n",
    "    # Calculate average reward\n",
    "    avg_reward = np.sum(r_)/n_\n",
    "    average.append(avg_reward)\n",
    "    \n",
    "    print('Training Episodes: {}  Average Reward: {:4.2f}  Total Average: {:4.2f}'.format(n, avg_reward, np.mean(average)))\n",
    "    \n",
    "    \n",
    "    sess.run(train_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "    \n",
    "    # Write TF summaries\n",
    "    summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\n",
    "    writer.add_summary(summary, step)\n",
    "    writer.flush()\n",
    "    \n",
    "    # Save Model\n",
    "    if step % 10 == 0:\n",
    "          print(\"SAVED MODEL\")\n",
    "          saver.save(sess, save_path, global_step=step)\n",
    "          \n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.init()\n",
    "state = game.get_state().screen_buffer\n",
    "state = preprocess_frame(state)\n",
    "prob, val = sess.run([aprob, value], feed_dict={X: state.reshape(1, 84, 84, 1)})\n",
    "\n",
    "print('Turn Right: {:4.2f}  Turn Left: {:4.2f}  Move Forward {:4.2f}'.format(prob[0][0],prob[0][2], prob[0][1]))\n",
    "print('Approximated State Value: {:4.4f}'.format(val[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See our Agent play üïπÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New render settings for replay\n",
    "game = DoomGame()\n",
    "game.load_config(\"health_gathering.cfg\")\n",
    "\n",
    "# New render settings for replay\n",
    "game.set_screen_resolution(ScreenResolution.RES_800X600)\n",
    "game.set_render_hud(True)\n",
    "\n",
    "# Replay can be played in any mode.\n",
    "game.set_mode(Mode.SPECTATOR)\n",
    "\n",
    "\n",
    "game.init()\n",
    "i = 500\n",
    "\n",
    "\n",
    "#sleep(2)\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    game.replay_episode(\"episode\" + str(i) + \"_rec.lmp\")\n",
    "\n",
    "    while not game.is_episode_finished():\n",
    "        s = game.get_state()\n",
    "\n",
    "        # Use advance_action instead of make_action.\n",
    "        game.advance_action()\n",
    "\n",
    "        r = game.get_last_reward()\n",
    "\n",
    "    print(\"Episode finished.\")\n",
    "    print(\"total reward:\", game.get_total_reward())\n",
    "    print(\"************************\")\n",
    "\n",
    "game.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gameplai]",
   "language": "python",
   "name": "conda-env-gameplai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
